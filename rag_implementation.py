# -*- coding: utf-8 -*-
"""Rag_implementation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FvvO7qs9mHkhkDBUD07hpZwqaIRwvmsk
"""

!pip install pypdf

!pip install -q transformers einops accelerate langchain bitsandbytes

!pip install install sentence_transformers

!pip install llama_index

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
from llama_index.core.prompts.prompts import SimpleInputPrompt
!pip install llama-index-llms-huggingface
from llama_index.llms.huggingface import HuggingFaceLLM

documents = SimpleDirectoryReader("/content/drive/MyDrive/Data").load_data()

system_prompt="""
You are a proposal document writer. Based on the context and information you have, you have to create a 5 paragraph proposal document based on the proposal document examples I have given. This has to be professionally written and formatted neatly.
"""

query_wrapper_prompt=SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")

!huggingface-cli login

!pip install accelerate
!pip install -i https://pypi.org/simple/ bitsandbytes

!pip install torch
import torch
from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=200.0)
llm = HuggingFaceLLM(

    ## context Window is the amount that the model will actually be able to take info from
    context_window=8192,
    ## max_new_tokens represents max number of tokens that can be outtputed
    max_new_tokens=1024,

    ## run it with different temperatures to see how the outputs are different from one another
    ## anything between 0 and 2
    ## based on testing I saw that I got the best results with a temp of .7
    generate_kwargs={"temperature":1.0,"do_sample":False},

    ## these were pre-defined if ever want to be referenced
    system_prompt = system_prompt,
    query_wrapper_prompt = query_wrapper_prompt,

    ## The Type of LLM that I am using
    tokenizer_name="meta-llama/Llama-2-7b-chat-hf",
    model_name="meta-llama/Llama-2-7b-chat-hf",
    device_map="auto", ## lets it auto decide whether it runs on CPU or GPU

    model_kwargs = {"torch_dtype": torch.float32, "load_in_8bit": True}
)

!pip install tensorflow
from tensorflow.keras import backend as K
K.clear_session()

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index.core import service_context
from llama_index.legacy.embeddings.langchain import LangchainEmbedding

service_context=ServiceContext.from_defaults(
    chunk_size=1024,
    llm=llm,
    embed_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"))
)

index=VectorStoreIndex.from_documents(documents,service_context=service_context)

query_engine=index.as_query_engine()

response=query_engine.query("Provide an in depth essay that states everything that one should know after reading these documents. I want this response to be extensive. I also want it to be readable.")

print(response)